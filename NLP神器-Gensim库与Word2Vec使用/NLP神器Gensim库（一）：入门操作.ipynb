{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前 言\n",
    "\n",
    "作为自然语言处理爱好者，大家都应该听说过或使用过大名鼎鼎的Gensim吧，这是一款具备多种功能的神器。\n",
    "Gensim是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。\n",
    "它支持包括TF-IDF，LSA，LDA，和word2vec在内的多种主题模型算法，\n",
    "支持流式训练，并提供了诸如相似度计算，信息检索等一些常用任务的API接口\n",
    "\n",
    "\n",
    "简单总结：\n",
    "\n",
    "所谓的word vector，就是指将单词向量化，将某个单词用特定的向量来表示。将单词转化成对应的向量以后，就可以将其应用于各种机器学习的算法中去。一般来讲，词向量主要有两种形式，分别是稀疏向量和密集向量。\n",
    "\n",
    "所谓稀疏向量，又称为one-hot representation，就是用一个很长的向量来表示一个词，向量的长度为词典的大小N，向量的分量只有一个1，其他全为0，1的位置对应该词在词典中的索引[1]。举例来说，如果有一个词典[“面条”,”方便面”,”狮子”]，那么“面条”对应的词向量就是[1,0,0]，“方便面”对应的词向量就是[0,1,0]。这种表示方法不需要繁琐的计算，简单易得，但是缺点也不少，比如长度过长（这会引发维数灾难），以及无法体现出近义词之间的关系，比如“面条”和“方便面”显然有非常紧密的关系，但转化成向量[1,0,0]和[0,1,0]以后，就看不出两者有什么关系了,因为这两个向量相互正交。当然了，用这种稀疏向量求和来表示文档向量效果还不错，清华的长文本分类工具THUCTC使用的就是此种表示方法\n",
    "\n",
    "至于密集向量，又称distributed representation，即分布式表示。最早由Hinton提出，可以克服one-hot representation的上述缺点，基本思路是通过训练将每个词映射成一个固定长度的短向量，所有这些向量就构成一个词向量空间，每一个向量可视为该空间上的一个点[1]。此时向量长度可以自由选择，与词典规模无关。这是非常大的优势。还是用之前的例子[“面条”,”方便面”,”狮子”]，经过训练后，“面条”对应的向量可能是[1,0,1,1,0],而“方便面”对应的可能是[1,0,1,0,0]，而“狮子”对应的可能是[0,1,0,0,1]。这样“面条”向量乘“方便面”=2，而“面条”向量乘“狮子”=0 。这样就体现出面条与方便面之间的关系更加紧密，而与狮子就没什么关系了。这种表示方式更精准的表现出近义词之间的关系，比之稀疏向量优势很明显。\n",
    "\n",
    "回过头来看word2vec，其实word2vec做的事情很简单，大致来说，就是构建了一个多层神经网络，然后在给定文本中获取对应的输入和输出，在训练过程中不断修正神经网络中的参数，最后得到词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "sys.version_info(major=3, minor=6, micro=12, releaselevel='final', serial=0)\n",
      "matplotlib 3.3.2\n",
      "numpy 1.16.4\n",
      "pandas 1.1.5\n",
      "sklearn 0.23.2\n",
      "tensorflow 1.13.1\n",
      "tensorflow._api.v1.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os \n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "\n",
    "for module in mpl,np,pd, sklearn,tf,keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一句话，Gensim中的Word2Vec类就是用来训练词向量的，这个类实现了词向量训练的两种基本模型skip-gram和CBOW，可以通过后面的参数设置来选择。\n",
    "\n",
    "    Skip-Gram models：输入为单个词，输出目标为多个上下文单词；\n",
    "    CBOW models：输入为多个上下文单词，输出目标为一个单词；\n",
    "    \n",
    " 我们从上面可以看出，无论是Skip-Gram models还是CBOW models基本的单元都是词，那么我们获取到的语料，必须要经过分词处理以后才能用于词向量的训练语料。\n",
    "\n",
    "但是，在Gensim这个模块中训练词向量的方法还有很多：\n",
    "\n",
    "    gensim.models.doc2vec.Doc2Vec,gensim.models.fasttext.FastText,gensim.models.wrappers.VarEmbed等等都能得到词向量。\n",
    "\n",
    "gensim是一款强大的自然语言处理工具，里面包括N多常见模型：\n",
    "- 基本的语料处理工具\n",
    "- LSI\n",
    "- LDA   主题文档的模型\n",
    "- HDP\n",
    "- DTM\n",
    "- DIM\n",
    "- TF-IDF\n",
    "- Word2Vec、paragraph2vec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Word2Vec类初始化参数:\n",
    "\n",
    "注：参数表这一列，等号右边的值表示默认值\n",
    "参数表含义\n",
    "    sentences=None\t语料句子，必须是一个可迭代的(参见后面例子）\n",
    "    size=100\t训练后词向量的维度（一般而言，合理的参数值会设置在0~100之间）\n",
    "    alpha=0.025\t训练网络的初始学习率，之后会线性降低\n",
    "    min_alpha=0.0001\t降低到最小的学习率\n",
    "    window=5\t当前词和预测词之间的最大间隔\n",
    "    min_count=5\t忽略词频＜5的词语\n",
    "    max_vocab_size=None\t限制最大词数，防止内存溢出\n",
    "    workers=3\t设置线程数，越高训练速度（前提是你有这么多）\n",
    "    sg=0\t训练模型的选择，1表示skip-gram，0表示CBOW\n",
    "    hs=0\t训练网络代价函数的选择\n",
    "    iter=5\t迭代次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练及保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim 是一个通过衡量词组（或更高级结构，如整句或文档）模式来挖掘文档语义结构的工具\n",
    "三大核心概念：文集（语料）–>向量–>模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# pip install gensim安装好库后，即可导入使用\n",
    "# 示例1\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#只需要给Word2Vec类赋上参数，就可以直接训练了。其中common_texts是一段内置的语料，如下\n",
    "print(common_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到整体格式是[['A','B'],['C','D','E']]，其中这个list表示所有的文本（此处表示2个文本，里面的A,B等表示单词，如果是中文则表示分词的结果，后面也会用中文演示）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [['human', 'interface', 'computer'],\n",
    "['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "['eps', 'user', 'interface', 'system'],\n",
    "['system', 'human', 'system', 'eps'],\n",
    "['user', 'response', 'time'],\n",
    "['trees'],\n",
    "['graph', 'trees'],\n",
    "['graph', 'minors', 'trees'],\n",
    "['graph', 'minors', 'survey']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# model的向量是字典格式\n",
    "train_models = Word2Vec(texts, size = 100, window = 5, min_count = 1, workers = 3)\n",
    "train_models.save('./data/MyModel')\n",
    "\n",
    "# 将模型保存成文本，model.wv.save_word2vec_format()来进行模型的保存的话，会生成一个模型文件。\n",
    "# 里边存放着模型中所有词的词向量。这个文件中有多少行模型中就有多少个词向量。\n",
    "# 输出txt格式的数据，(一共有12个词向量，每个词向量100维)\n",
    "train_models.wv.save_word2vec_format('./data/mymodel.txt', binary = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save和save_word2vec_format,\n",
    "两者之间的相同点就是：都可以复用，即载入之后可以得到对应单词的词向量；\n",
    "不同点是：save保存的模型，载入之后可以继续在此基础上接着训练（后文会介绍），而format_save保存的模型不能，但有个好处就是设置binary=False则保存后的结果可以直接打开查看(一共有12个词向量，每个词向量100维)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 100\n",
      "\n",
      "system -0.00037184713 -0.0009399182 -0.0019755496 0.0028610718 0.0036305669 -0.0030484397 -0.0013936844 -0.0026018433 -0.00024051622 0.0024654588 0.002391837 4.8928934e-05 -0.0022977763 0.0027941805 0.004522777 -0.002606438 0.0019814991 -0.0020462577 -0.0025924942 -0.0037542353 -0.004020849 -0.0037770048 -0.0008118235 -0.0008461102 0.0021810236 -0.0035013775 0.003047179 -0.0033355686 0.00044874742 -0.0013311415 -0.0014471688 -0.0035278546 -0.0031719813 -0.0046979166 -0.0018826766 -0.002869913 0.0036466985 -0.0034138472 0.004662448 -0.0014318564 -0.00277513 -0.0046495297 0.0018640485 -0.0046703643 -0.001519623 0.0042177206 -0.0038973563 -0.00048293607 -0.0022529082 7.716712e-05 -0.0016006421 -0.0014990068 -0.003633369 -0.0033900037 0.0035776657 -0.0049347444 -0.0021816946 0.00047621547 0.0015204913 0.0037792732 -0.0024766915 -0.0015134101 0.0008302899 -0.0028516366 -0.0024063224 -0.00067469035 0.0025503745 -0.001860866 -0.0030900452 0.004611408 0.0019481141 -0.0024335023 -0.0041649146 0.0008235008 -0.00022761726 0.003256772 -0.004876926 -0.0006359506 -0.0045309663 -0.0017890941 -0.002419049 0.0041382997 -0.0040064952 -0.0029851103 0.003182326 0.00036981766 0.00050642417 -0.0039072335 -0.000404607 0.0018813239 -0.002539914 -0.003647682 0.0047723013 -0.002711267 0.0005536606 -0.004767684 0.0039691934 0.0042931805 0.0022020605 -0.0016073353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./data/mymodel.txt', 'r') as f:\n",
    "    print(f.readline())\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入模型和使用（英文）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human :        Vocab(count:2, index:4, sample_int:579459575)\n",
      "interface :        Vocab(count:2, index:5, sample_int:579459575)\n",
      "computer :        Vocab(count:2, index:6, sample_int:579459575)\n",
      "survey :        Vocab(count:2, index:7, sample_int:579459575)\n",
      "user :        Vocab(count:3, index:1, sample_int:463795800)\n",
      "system :        Vocab(count:4, index:0, sample_int:396841800)\n",
      "response :        Vocab(count:2, index:8, sample_int:579459575)\n",
      "time :        Vocab(count:2, index:9, sample_int:579459575)\n",
      "eps :        Vocab(count:2, index:10, sample_int:579459575)\n",
      "trees :        Vocab(count:3, index:2, sample_int:463795800)\n",
      "graph :        Vocab(count:3, index:3, sample_int:463795800)\n",
      "minors :        Vocab(count:2, index:11, sample_int:579459575)\n"
     ]
    }
   ],
   "source": [
    "# 导入模型\n",
    "\n",
    "#示例 2  查看词表相关信息\n",
    "# model.wv.vocab #获取词表中的总词数\n",
    "\n",
    "model = Word2Vec.load('./data/MyModel')\n",
    "for key in model.wv.vocab:\n",
    "    print(key ,':       ', model.wv.vocab[key])  #词表中的词，频度，以及索引位置， "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Word2Vec(vocab=12, size=100, alpha=0.025)\n",
      "[ 4.9716486e-03 -3.0522095e-03 -3.4746842e-03  1.8339788e-03\n",
      "  3.3757681e-04  4.8922976e-03  3.4507182e-03  3.3485841e-03\n",
      " -3.8536128e-03  2.1396433e-03 -3.8845125e-03 -1.9657731e-03\n",
      "  8.2060107e-04  6.1265373e-04  6.1835820e-04  1.1991938e-03\n",
      " -1.8903082e-03  2.9800097e-03 -1.8302508e-03 -4.7187353e-03\n",
      " -2.5183442e-03 -5.9188961e-04  2.5387101e-03 -6.9486327e-04\n",
      " -4.4346820e-03  2.4674842e-03  2.5721465e-03  8.3059515e-04\n",
      "  4.6638283e-03  4.5282701e-03 -2.7610930e-03 -1.3592412e-03\n",
      "  3.1772144e-03  5.0937582e-04 -1.8634959e-03  1.6900046e-03\n",
      " -3.8834477e-03  4.8440578e-03  3.7525713e-03 -1.5786855e-03\n",
      "  2.1969450e-03  2.2236265e-03 -4.7302246e-03 -1.9446870e-03\n",
      " -3.1025417e-03  1.1209847e-03 -8.0657302e-04 -2.6665109e-03\n",
      " -1.3545981e-03 -3.2565377e-03  2.6251795e-03  1.7857554e-04\n",
      " -4.2432873e-03  3.3073262e-03 -2.2579639e-03 -2.2405197e-03\n",
      "  4.6394326e-04 -2.8829768e-03  1.2882162e-03 -4.7860472e-03\n",
      " -1.3488799e-03  4.4502281e-03 -8.1358943e-04  2.8783144e-04\n",
      "  1.3997977e-03 -3.6083681e-03 -1.9151547e-03 -7.7964540e-04\n",
      " -4.0911096e-03  3.9341887e-03 -2.9613639e-03 -1.5609282e-04\n",
      " -3.1375526e-03 -1.2564638e-03  2.8702067e-03 -1.3190877e-03\n",
      " -4.8243944e-03 -1.1606918e-03  2.8323319e-03 -1.9855832e-03\n",
      " -2.8889065e-03 -3.8384460e-04 -4.3590036e-03 -3.9227288e-03\n",
      " -6.7294145e-04 -3.4958893e-03  3.4054497e-03 -1.9104534e-03\n",
      "  4.9540349e-03  2.1941864e-03  2.6249459e-03 -3.2976677e-03\n",
      "  3.4183513e-03 -8.6655992e-04 -9.9057295e-05 -4.2225695e-03\n",
      " -1.0592949e-03 -3.9667678e-03  3.5661315e-03 -8.9960941e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\develop\\Anaconda3\\envs\\DL_interview\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 示例3  获取对应的词向量及维度\n",
    "\n",
    "model = Word2Vec.load('./data/MyModel')\n",
    "print(model.wv.vector_size) # 输出向量的维度\n",
    "print(model) # 输入Word2Vec(texts, size = 100, window = 5, min_count = 1, workers = 3)，输出Word2Vec(vocab=12, size=100, alpha=0.025)\n",
    "print(model['human'])\n",
    "\n",
    "# print(model['good']) # 输出 KeyError: \"word 'good' not in vocabulary\" #在取词向量之前一定要先判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.015187759\n",
      "-0.025646772\n"
     ]
    }
   ],
   "source": [
    "# 示例4  常用方法\n",
    "\n",
    "#---------------4.1  计算两个词的相似度（余弦距离）--------\n",
    "mdoel = Word2Vec.load('./data/MyModel')\n",
    "\n",
    "# 结果越大越相似（此处由于维度太小，所以结果好像不怎么准确）\n",
    "print(mdoel.wv.similarity('human', 'interface')) \n",
    "print(mdoel.wv.similarity('human', 'user')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0151877589523792\n",
      "1.025646772235632\n"
     ]
    }
   ],
   "source": [
    "#---------------4.2  计算两个词的距离-------- \n",
    "model = Word2Vec.load('./data/MyModel')\n",
    "\n",
    "# 结果越大越不相似\n",
    "print(model.wv.distance('human', 'interface'))\n",
    "print(model.wv.distance('human', 'user'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('system', 0.08545435965061188), ('eps', 0.031795158982276917), ('trees', 0.027651473879814148)]\n"
     ]
    }
   ],
   "source": [
    "#---------------4.3  取与给定词最相近的topn个词--------\n",
    "\n",
    "model = Word2Vec.load('./data/MyModel')\n",
    "\n",
    "print(model.wv.most_similar(['human'], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\develop\\Anaconda3\\envs\\DL_interview\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "#---------------4.4  找出与其他词差异最大的词 ------------\n",
    "model = Word2Vec.load('./data/MyModel')\n",
    "\n",
    "print(model.wv.doesnt_match(['trees', 'response', 'minors']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入模型并继续训练\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入模型并继续训练意思是，之前训练好了一个词向量模型，可能训练时间不足，或者又有了新的数据，那么此时就可以在原来的基础上接着训练而不用从头再来。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料数： 9\n",
      "词表长度： 12\n"
     ]
    }
   ],
   "source": [
    "train_models = Word2Vec(texts, size = 100, window = 5, min_count = 1, workers = 3)\n",
    "train_models.save('./data/MyModel2')\n",
    "\n",
    "print('语料数：', model.corpus_count) #文本长度\n",
    "print('词表长度：', len(model.wv.vocab)) # 词汇表总共12个词，每个词的向量维度为100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料数： 9\n",
      "词表长度： 12\n",
      "追加后---语料数： 10\n",
      "追加后---词表长度： 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\develop\\Anaconda3\\envs\\DL_interview\\lib\\site-packages\\ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    }
   ],
   "source": [
    "#-------------增量训练------------\n",
    "\n",
    "texts2 = [['human', 'interface', 'computer'],\n",
    "['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "['eps', 'user', 'interface', 'system'],\n",
    "['system', 'human', 'system', 'eps'],\n",
    "['user', 'response', 'time'],\n",
    "['trees'],\n",
    "['graph', 'trees'],\n",
    "['graph', 'minors', 'trees'],\n",
    "['graph', 'minors', 'survey'],\n",
    "['a', 'aa', 'aaa']] #新增语料['a', 'aa', 'aaa']\n",
    "\n",
    "model = Word2Vec.load('./data/MyModel2')\n",
    "print('语料数：', model.corpus_count)\n",
    "print('词表长度：', len(model.wv.vocab)) \n",
    "\n",
    "# 继续训练\n",
    "model.build_vocab(sentences=texts2, update=True)\n",
    "model.train(sentences=texts2, total_examples=model.corpus_count, epochs=model.iter)\n",
    "model.save('./data/MyModel2_add')\n",
    "\n",
    "print('追加后---语料数：', model.corpus_count)\n",
    "print('追加后---词表长度：', len(model.wv.vocab)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入模型和使用（中文）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本为中文的时候需要分词，并去掉停用词\n",
    "chinese_texts = [['我', '是', '中国人'], ['自然语言', '处理'], ['人工智能', '处置']]\n",
    "\n",
    "train_models = Word2Vec(chinese_texts, size = 5, window = 5, min_count = 1, workers = 3)\n",
    "train_models.save('./data/MyModel_ch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料数： 3\n",
      "词表长度： 7\n",
      "处置 对应的词向量为： [-0.02640922  0.04038697  0.01784483  0.02368114 -0.00472705]\n",
      "处理 对应的词向量为： [-0.04380829  0.0815771   0.02407022 -0.08469722 -0.08620078]\n",
      "处理 和 处置 的距离（余弦距离）： 0.3747511\n",
      "处理 和 处置 的距离（余弦距离）： 0.625248908996582\n",
      "与 处理 最相近的三个词： [('处置', 0.3747510612010956), ('是', 0.2171233892440796), ('我', 0.19002287089824677)]\n",
      "我', '是', '中国人'中最与众不同的词： 是\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\develop\\Anaconda3\\envs\\DL_interview\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "D:\\develop\\Anaconda3\\envs\\DL_interview\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n",
      "D:\\develop\\Anaconda3\\envs\\DL_interview\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load('./data/MyModel_ch')\n",
    "\n",
    "print('语料数：', model.corpus_count)\n",
    "print('词表长度：', len(model.wv.vocab)) \n",
    "\n",
    "print('处置 对应的词向量为：', model['处置'])\n",
    "print('处理 对应的词向量为：', model['处理'])\n",
    "\n",
    "print('处理 和 处置 的距离（余弦距离）：', model.wv.similarity('处理', '处置'))\n",
    "print('处理 和 处置 的距离（余弦距离）：', model.wv.distance('处理', '处置'))\n",
    "\n",
    "print('与 处理 最相近的三个词：', model.wv.similar_by_word('处理', topn = 3))\n",
    "print(\"我', '是', '中国人'中最与众不同的词：\", model.wv.doesnt_match(['我', '是', '中国人']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL_interview]",
   "language": "python",
   "name": "conda-env-DL_interview-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
